{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba28da14-561e-436c-86d3-86795f95fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af58f53f-16c5-4a44-bfff-d4d31dc7e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicData(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        super(ToxicData, self).__init__()\n",
    "        \n",
    "        self.dataframe = pd.read_csv(path)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        pair = self.dataframe.iloc[idx]\n",
    "        less_toxic = pair['less_toxic']\n",
    "        more_toxic = pair['more_toxic']\n",
    "        \n",
    "        more_toxic = tokenizer.encode_plus(more_toxic, add_special_tokens=True, padding = 'max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "        less_toxic = tokenizer.encode_plus(less_toxic, add_special_tokens=True, padding = 'max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        ##Squeezing because it adds a sequence dimension which is not needed in the default\n",
    "        less_toxic_tokens =  less_toxic['input_ids'].squeeze(0)\n",
    "        less_toxic_attn_mask = less_toxic['attention_mask'].squeeze(0)\n",
    "        more_toxic_tokens = more_toxic['input_ids'].squeeze(0)\n",
    "        more_toxic_attn_mask = more_toxic['attention_mask'].squeeze(0)\n",
    "                \n",
    "        targets = torch.ones(1).squeeze(0) ##If this is 1 then more toxic should be first input to MarginRankingLoss else use -1.\n",
    "        \n",
    "        return {'less_toxic_tokens': less_toxic_tokens, 'less_toxic_attn_mask': less_toxic_attn_mask, 'more_toxic_tokens': more_toxic_tokens, 'more_toxic_attn_mask': more_toxic_attn_mask, 'targets': targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5b1c04-2095-413f-8acd-72353d768441",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataPath = './data/validation_data.csv'\n",
    "# df = pd.read_csv('./data/validation_data.csv')\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased', cache_dir = './input/tokenizer')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "dataset = ToxicData(dataPath, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e7df2c-56b2-4325-946e-413cee9dba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, attention_mask):\n",
    "        \n",
    "        x = self.bert(x, attention_mask, output_hidden_states=False)\n",
    "        x = self.dropout(x[1])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3598104-2abe-456c-ba63-d687eb74908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# model = transformers.AutoModel.from_pretrained('bert-base-cased', cache_dir = './input/model/')\n",
    "# model = transformers.AutoModel.from_pretrained('bert-base-cased')\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc666978-9b3b-4c52-927b-7c8944ae6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    less_toxic_tokens = batch['less_toxic_tokens'].to(device)\n",
    "    less_toxic_attn_mask = batch['less_toxic_attn_mask'].to(device)\n",
    "    \n",
    "    more_toxic_tokens = batch['more_toxic_tokens'].to(device)\n",
    "    more_toxic_attn_mask = batch['more_toxic_attn_mask'].to(device)\n",
    "    \n",
    "    targets = batch['targets'].to(device)\n",
    "\n",
    "    less_toxic_score = model(less_toxic_tokens, less_toxic_attn_mask)\n",
    "    more_toxic_score = model(more_toxic_tokens, more_toxic_attn_mask)\n",
    "\n",
    "    batch_loss = criterion(more_toxic_score, less_toxic_score, targets)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9991d2d-b9cf-4fd3-aa22-b741ed765882",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_step(dataloader):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):    \n",
    "\n",
    "        less_toxic_tokens = batch['less_toxic_tokens'].to(device)\n",
    "        less_toxic_attn_mask = batch['less_toxic_attn_mask'].to(device)\n",
    "        \n",
    "        more_toxic_tokens = batch['more_toxic_tokens'].to(device)\n",
    "        more_toxic_attn_mask = batch['more_toxic_attn_mask'].to(device)\n",
    "        \n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        less_toxic_score = model(less_toxic_tokens, less_toxic_attn_mask)\n",
    "        more_toxic_score = model(more_toxic_tokens, more_toxic_attn_mask)\n",
    "\n",
    "        batch_loss = criterion(more_toxic_score, less_toxic_score, targets)\n",
    "        total_loss += batch_loss.item()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffb914f-f6c5-4d68-a591-f50b8b10ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MarginRankingLoss(margin = 1.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1348800b-3bf3-4d53-a2ac-aa0c875e1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d23d59-8f09-48ed-a2a8-8cd9f5f5426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 1/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.7239\n",
      "\n",
      "Epoch: 2/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.6166\n",
      "\n",
      "Epoch: 3/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.5597\n",
      "\n",
      "Epoch: 4/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.5265\n",
      "\n",
      "Epoch: 5/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.5013\n",
      "\n",
      "Epoch: 6/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.4866\n",
      "\n",
      "Epoch: 7/20\n",
      "470/471 [========>.] - ETA: 0s - train_loss: 0.4776\n",
      "\n",
      "Epoch: 8/20\n",
      "273/471 [====>.....] - ETA: 1:19 - train_loss: 0.4557"
     ]
    }
   ],
   "source": [
    "train_per_epoch = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    kbar = pkbar.Kbar(target = train_per_epoch, epoch=epoch, num_epochs=num_epochs, width = 10, always_stateful=False)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):    \n",
    "\n",
    "        batch_loss = train_step(batch)\n",
    "        \n",
    "        ########################################################################\n",
    "        kbar.update(idx, values= [(\"train_loss\", batch_loss)])\n",
    "        ########################################################################\n",
    "    \n",
    "    ########################################################################\n",
    "    # val_loss = val_step(dataloader)\n",
    "    # kbar.add(1, values = [(\"val_loss\", val_loss)])\n",
    "    ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b49519-09f1-47c3-8a5c-54601faf0176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
