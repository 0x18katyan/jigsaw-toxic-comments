{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba28da14-561e-436c-86d3-86795f95fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af58f53f-16c5-4a44-bfff-d4d31dc7e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicData(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        super(ToxicData, self).__init__()\n",
    "        \n",
    "        self.dataframe = pd.read_csv(path)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        pair = self.dataframe.iloc[idx]\n",
    "        less_toxic = pair['less_toxic']\n",
    "        more_toxic = pair['more_toxic']\n",
    "        \n",
    "        more_toxic = tokenizer.encode_plus(more_toxic, add_special_tokens=True, padding = 'max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "        less_toxic = tokenizer.encode_plus(less_toxic, add_special_tokens=True, padding = 'max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        ##Squeezing because it adds a sequence dimension which is not needed in the default\n",
    "        less_toxic_tokens =  less_toxic['input_ids'].squeeze(0)\n",
    "        less_toxic_attn_mask = less_toxic['attention_mask'].squeeze(0)\n",
    "        more_toxic_tokens = more_toxic['input_ids'].squeeze(0)\n",
    "        more_toxic_attn_mask = more_toxic['attention_mask'].squeeze(0)\n",
    "                \n",
    "        targets = torch.ones(1).squeeze(0) ##If this is 1 then more toxic should be first input to MarginRankingLoss else use -1.\n",
    "        \n",
    "        return {'less_toxic_tokens': less_toxic_tokens, 'less_toxic_attn_mask': less_toxic_attn_mask, 'more_toxic_tokens': more_toxic_tokens, 'more_toxic_attn_mask': more_toxic_attn_mask, 'targets': targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5b1c04-2095-413f-8acd-72353d768441",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataPath = './data/validation_data.csv'\n",
    "# df = pd.read_csv('./data/validation_data.csv')\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased', cache_dir = './input/tokenizer')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "dataset = ToxicData(dataPath, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size = 128, shuffle = True, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95eed52f-22f8-4fc0-b311-825b0c16b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitBert(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(LitBert, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "            \n",
    "    def forward(self, x, attention_mask):\n",
    "        \n",
    "        x = self.bert(x, attention_mask, output_hidden_states=False)\n",
    "        x = self.dropout(x[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "            \n",
    "        less_toxic_tokens = batch['less_toxic_tokens']\n",
    "        less_toxic_attn_mask = batch['less_toxic_attn_mask']\n",
    "        more_toxic_tokens = batch['more_toxic_tokens']\n",
    "        more_toxic_attn_mask = batch['more_toxic_attn_mask']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        less_toxic_score = model(less_toxic_tokens, less_toxic_attn_mask)\n",
    "        more_toxic_score = model(more_toxic_tokens, more_toxic_attn_mask)\n",
    "\n",
    "        loss = F.margin_ranking_loss(more_toxic_score, less_toxic_score, targets, margin = 1.0)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr = 3e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3598104-2abe-456c-ba63-d687eb74908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# model = transformers.AutoModel.from_pretrained('bert-base-cased', cache_dir = './input/model/')\n",
    "# model = transformers.AutoModel.from_pretrained('bert-base-cased')\n",
    "model = LitBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba4c72d-5888-4f4a-9876-6c46eb3a5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84ce50c-2700-4231-a832-0b87a5a64e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params\n",
      "--------------------------------------\n",
      "0 | bert    | BertModel | 108 M \n",
      "1 | fc      | Linear    | 769   \n",
      "2 | dropout | Dropout   | 0     \n",
      "--------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "216.622   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542006ae5dad41f2a08d542b9a69c040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = TensorBoardLogger(save_dir = os.getcwd(), version=1, name='lightning_logs')\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, precision = 16, gpus = 1, deterministic = True, logger=logger)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9991d2d-b9cf-4fd3-aa22-b741ed765882",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_step(dataloader):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):    \n",
    "\n",
    "        less_toxic_tokens = batch['less_toxic_tokens'].to(device)\n",
    "        less_toxic_attn_mask = batch['less_toxic_attn_mask'].to(device)\n",
    "        \n",
    "        more_toxic_tokens = batch['more_toxic_tokens'].to(device)\n",
    "        more_toxic_attn_mask = batch['more_toxic_attn_mask'].to(device)\n",
    "        \n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        less_toxic_score = model(less_toxic_tokens, less_toxic_attn_mask)\n",
    "        more_toxic_score = model(more_toxic_tokens, more_toxic_attn_mask)\n",
    "\n",
    "        batch_loss = criterion(more_toxic_score, less_toxic_score, targets)\n",
    "        total_loss += batch_loss.item()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1348800b-3bf3-4d53-a2ac-aa0c875e1bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b49519-09f1-47c3-8a5c-54601faf0176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
